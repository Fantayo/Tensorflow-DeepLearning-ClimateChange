
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning with TensorFlow\n",
    "\n",
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/TheAIDojo/AI_4_Climate_Bootcamp/blob/main/Week 01 - Review and Intro to Deep Learning/1. Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
    "\n",
    "TensorFlow is an open-source library for deep learning that was developed by Google. It provides a flexible and powerful framework for building, training, and deploying deep learning models. TensorFlow also has a large and active community, which provides a wealth of resources and tutorials for learning and using the library.\n",
    "\n",
    "In this notebook, we will be learning how to use TensorFlow to build and train deep learning models for climate change prediction. We will also be learning about regularization techniques and how they can be used to prevent overfitting and improve the generalization of deep learning models.\n",
    "\n",
    "Helpful External Resources:\n",
    "* [TensorFlow Guides](https://www.tensorflow.org/guide)\n",
    "* [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)"
   ],
   "metadata": {
    "id": "uBVSz-GUz2Us",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "* [Tensors](#tensors)\n",
    "* [Layers](#layers)\n",
    "* [Activation Functions](#activation-functions)\n",
    "* [Models](#models)\n",
    "* [Losses](#losses)\n",
    "* [Optimizers](#optimizers)\n",
    "* [Putting it All Together](#examples)"
   ],
   "metadata": {
    "id": "FsyfJp872iWq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nYWU9Ihcz0SO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors <a class=\"anchor\" id=\"tensors\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "\n",
    "Tensors are the fundamental building blocks of TensorFlow. They are multi-dimensional arrays that can represent scalars, vectors, matrices, and higher-dimensional arrays of data. Tensors are similar to numpy arrays and can be used in similar ways, but they also have some unique features that make them well suited for deep learning.\n",
    "\n",
    "In TensorFlow, tensors are used to represent the inputs and outputs of a model, as well as the model's parameters. They are also used to represent gradients, which are used in the training process. Tensors can be created, manipulated and transformed with a variety of functions provided by TensorFlow.\n",
    "\n",
    "Let's take a look at how to create and manipulate tensors in TensorFlow"
   ],
   "metadata": {
    "id": "le9lBOC02PmI",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a tensor with shape (2,3) and filled with zeros\n",
    "tensor_a = tf.zeros((2, 3))\n",
    "print(\"Tensor A:\\n\", tensor_a, \"\\n\")\n",
    "\n",
    "# create a tensor with shape (2,3) and filled with ones\n",
    "tensor_b = tf.ones((2, 3))\n",
    "print(\"Tensor B:\\n\", tensor_b, \"\\n\")\n",
    "\n",
    "# create a tensor with shape (2,3) and filled with a constant value\n",
    "tensor_c = tf.fill((3, 2), 4.0)\n",
    "print(\"Tensor C:\\n\", tensor_c, \"\\n\")\n",
    "\n",
    "# create a tensor from a numpy array\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_d = tf.constant(numpy_array)\n",
    "print(\"Tensor D:\\n\", tensor_d)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQNdLVwd2ml0",
    "outputId": "9c4c9b21-b15e-478c-a566-c252f42ae1c5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor A:\n",
      " tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "Tensor B:\n",
      " tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "Tensor C:\n",
      " tf.Tensor(\n",
      "[[4. 4.]\n",
      " [4. 4.]\n",
      " [4. 4.]], shape=(3, 2), dtype=float32) \n",
      "\n",
      "Tensor D:\n",
      " tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# addition of two tensors\n",
    "addition = tf.add(tensor_a, tensor_b)\n",
    "print(\"Tensor A + Tensor B:\\n\", addition, \"\\n\")\n",
    "\n",
    "# matrix multiplication of two tensors\n",
    "# note that in matrix multiplication, the number of columns of the first matrix must be equal to the number of rows of the second matrix.\n",
    "matmul = tf.matmul(tensor_b, tensor_c)\n",
    "print(\"Tensor B . Tensor C:\\n\", matmul, \"\\n\")\n",
    "\n",
    "# transpose of a tensor\n",
    "transpose = tf.transpose(tensor_d)\n",
    "print(\"Tensor D Transposed:\\n\", transpose, \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-wi-6Ul4BVz",
    "outputId": "67aa2c73-2c39-4a57-decb-36b0fc088edb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor A + Tensor B:\n",
      " tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "Tensor B . Tensor C:\n",
      " tf.Tensor(\n",
      "[[12. 12.]\n",
      " [12. 12.]], shape=(2, 2), dtype=float32) \n",
      "\n",
      "Tensor D Transposed:\n",
      " tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int64) \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# get shape of a tensor\n",
    "shape = tensor_a.shape\n",
    "print(\"Tensor A Shape:\", shape, \"\\n\")\n",
    "\n",
    "# get specific element of a tensor\n",
    "element = tensor_d[1, 2]\n",
    "print(\"Element at Tensor D index [1,2]:\", element)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfrsd0pX4e8m",
    "outputId": "be5038bc-ebf9-40df-ddec-408e5fb85424",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor A Shape: (2, 3) \n",
      "\n",
      "Element at Tensor D index [1,2]: tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers <a class=\"anchor\" id=\"layers\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "\n",
    "In TensorFlow, the `tf.keras` module provides a high-level API for building and training deep learning models. The `tf.keras.layers` module provides a variety of pre-built layers that can be used to construct neural networks. These layers are the building blocks of a neural network and can be combined to create complex architectures.\n",
    "\n",
    "`tf.keras.layers.Dense` layer is used to create fully connected layers in a neural network. It takes in the number of units as an argument and creates that many neurons in the layer.\n"
   ],
   "metadata": {
    "id": "4XiW4FSG6gNm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dense layer with 4 units\n",
    "dense_layer = tf.keras.layers.Dense(4)\n",
    "\n",
    "# create a tensor with shape (2, 8) as input (2 input samples, each one with 8 features)\n",
    "input_tensor = tf.random.normal([2, 8])\n",
    "\n",
    "# pass the input tensor through the dense layer\n",
    "output_tensor = dense_layer(input_tensor)\n",
    "\n",
    "print(\"Output Tensor:\", output_tensor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mm-kOO8o6HYK",
    "outputId": "41680933-08f0-42d7-8987-6c386f2baaf1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output Tensor: tf.Tensor(\n",
      "[[ 0.05513961 -0.5339481   1.4474454  -0.7458242 ]\n",
      " [-0.25193113  0.12729234  0.7375244  -0.65112084]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# access the weights of the dense layer\n",
    "weights = dense_layer.weights\n",
    "\n",
    "# print the shape of the weights, these weights are randomly initalized\n",
    "print(\"Weights:\", weights)  # Weights Shape: [(8, 4), (4,)]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kQHmKXn7nE2",
    "outputId": "46891215-0c16-48b0-df32-f090d873b83b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weights: [<tf.Variable 'dense/kernel:0' shape=(8, 4) dtype=float32, numpy=\n",
      "array([[ 0.31018978, -0.14550912,  0.6132663 , -0.45733932],\n",
      "       [-0.30899405, -0.2611437 , -0.02609718, -0.33450773],\n",
      "       [ 0.10387498, -0.26177594,  0.2874294 , -0.05009168],\n",
      "       [ 0.48014635,  0.07783931,  0.6510461 ,  0.2682925 ],\n",
      "       [ 0.69828373, -0.41942048, -0.6542123 ,  0.2336461 ],\n",
      "       [-0.45531914,  0.1473034 , -0.513004  ,  0.1597507 ],\n",
      "       [-0.68696666,  0.24396586, -0.5050452 , -0.12076068],\n",
      "       [ 0.07181603, -0.27238378, -0.5672283 , -0.53586835]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Functions <a class=\"anchor\" id=\"activation-functions\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "Activation functions are an important component of neural networks and are used to introduce non-linearity into the model. They are applied element-wise to the output of a layer and are used to introduce non-linearity into the model. Activation functions are used to introduce non-linearity because a neural network composed of only linear layers would be equivalent to a linear model, which is not expressive enough to model complex patterns in data.\n",
    "\n",
    "Some common activation functions used in neural networks are:\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: ReLU is one of the most widely used activation functions. It is computationally efficient, easy to implement and does not saturate for positive values of x. It is mainly used in the hidden layers of neural networks.\n",
    "\n",
    "Code: `tf.nn.relu`\n",
    "\n",
    "Math: $f(x) = max(0, x)$\n",
    "\n",
    "\n",
    "**Sigmoid**: Sigmoid function maps the input to a value between 0 and 1. It is mainly used in the output layer of binary classification problems to get a probability value.\n",
    "\n",
    "Code: `tf.nn.sigmoid`\n",
    "\n",
    "Math: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "**Tanh (Hyperbolic Tangent)**: The tanh function maps the input to a value between -1 and 1. It is similar to sigmoid function but it is zero-centered, which means it has negative values as well. It is mainly used in the hidden layers of neural networks.\n",
    "\n",
    "Code: `tf.nn.tanh`\n",
    "\n",
    "Math: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "**Softmax**: The softmax function is mainly used in the output layer of multi-class classification problems to get a probability distribution over the classes.\n",
    "\n",
    "Code: `tf.nn.softmax`\n",
    "\n",
    "Math: $f(x_i) = \\frac{e^{x_i}}{\\sum(e^{x_j})}$\n"
   ],
   "metadata": {
    "id": "_DjgShC18y1Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dense layer with 4 units\n",
    "dense_layer_w_activation = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "\n",
    "# pass the input tensor through the dense layer\n",
    "output_tensor = dense_layer_w_activation(input_tensor)\n",
    "\n",
    "print(\n",
    "    \"Output Tensor:\", output_tensor\n",
    ")  # note that some of the outputs are zeros because we used ReLU"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxntfY-1-WDc",
    "outputId": "77291c33-2bc2-4773-8c1c-dfa1d8a5488b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output Tensor: tf.Tensor(\n",
      "[[0.2585567  0.         0.         0.37567353]\n",
      " [0.19427118 2.1795511  1.9128449  0.        ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models <a class=\"anchor\" id=\"models\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "In TensorFlow with Keras, we can stack multiple layers together to create a sequential model by using the `tf.keras.Sequential` class. This class allows us to create a linear stack of layers, where the output of one layer is used as the input for the next layer.\n",
    "\n",
    "When building models in TensorFlow with Keras, the way we build the model will depend on the type of output we are trying to predict. Here are some guidelines on how to build a model for different types of outputs:\n",
    "\n",
    "* Binary Classification: When the output is binary, it means that there are only two possible outcomes, such as true or false, yes or no, etc. In this case, the last layer of the model should be a single sigmoid neuron, which will output a value between 0 and 1, representing the probability of the positive class.\n",
    "\n",
    "* Multinomial Classification: When the output is multinomial, it means that there are more than two possible outcomes, such as red, green, blue, etc. In this case, the last layer of the model should be a softmax layer, which will output a probability distribution over the classes.\n",
    "* Regression: When the output is a single continuous value, such as a price, temperature, etc. In this case, the last layer of the model should be a single neuron with no activation function."
   ],
   "metadata": {
    "id": "qB_hSBU08HGU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create an empty sequential model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # specify the dimension of the input, in this case it's a vector of 8 feautres\n",
    "        tf.keras.layers.Input(shape=(8,)),\n",
    "        # a dense layer with 4 units and ReLU activation\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "        # a dense layer with 1 units and sigmoid activation\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGeJT_GU7kDn",
    "outputId": "9e5855da-c01c-4a2a-d66c-2bbb51abb732",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# pass the input tensor through the whole model\n",
    "model_output = model.predict(input_tensor)\n",
    "\n",
    "print(\n",
    "    \"Model Outputs:\", model_output\n",
    ")  # notice that the output for each sample is a single number between 0 and 1 because we used Sigmoid"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bye2hhtT7YfI",
    "outputId": "7d2c86fc-b5ba-48c3-e39b-0ead3fe2450f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 252ms/step\n",
      "Model Outputs: [[0.9021145 ]\n",
      " [0.74977976]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Losses <a class=\"anchor\" id=\"losses\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "A loss function is a function that measures the difference between the predicted output and the actual output. The goal of training a neural network is to minimize the loss function. There are several loss functions available in TensorFlow, and the appropriate loss function to use depends on the type of output and use case.\n",
    "\n",
    "Here are some commonly used loss functions in TensorFlow, along with the appropriate output type and use case:\n",
    "\n",
    "* **Binary Cross-Entropy**: This loss function is used for binary classification problems, where the output is a probability of one class. It is calculated as the negative log likelihood of the true labels given the predicted labels. In TensorFlow, the `tf.losses.BinaryCrossentropy` class can be used to calculate binary cross-entropy loss.\n",
    "\n",
    "* **Categorical Cross-Entropy**: This loss function is used for multi-class classification problems, where the output is a probability distribution over multiple classes. It is calculated as the negative log likelihood of the true labels given the predicted labels. In TensorFlow, the `tf.losses.CategoricalCrossentropy` class can be used to calculate categorical cross-entropy loss.\n",
    "\n",
    "* **Mean Absolute Error (MAE)**: This loss function is also used for regression problems, where the output is a single continuous value. It is calculated as the average of the absolute differences between the predicted values and the true values. In TensorFlow, the `tf.losses.MeanAbsoluteError` class can be used to calculate mean absolute error loss.\n",
    "\n",
    "* **Huber Loss**: This loss function is also used for regression problems, it is a combination of both MAE and MSE. It's less sensitive to outliers than MSE. In TensorFlow, the `tf.losses.Huber` class can be used to calculate Huber loss.\n"
   ],
   "metadata": {
    "id": "8Lo3sPBLA4mR",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers <a class=\"anchor\" id=\"optimizers\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "An optimizer is an algorithm used to update the model's parameters in order to minimize the loss function. There are several optimizers available in TensorFlow, and the appropriate optimizer to use depends on the specific problem and use case.\n",
    "\n",
    "Here are some commonly used optimizers in TensorFlow, along with a brief explanation of how they work:\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**: This is a simple optimization algorithm that updates the model's parameters based on the gradient of the loss function with respect to the parameters. It is computationally efficient and easy to implement, but can be sensitive to the learning rate and may get stuck in local minima.\n",
    "* **Adagrad**: This optimizer adapts the learning rate for each parameter based on the historical gradient information. The learning rate is increased for parameters that receive small gradients and decreased for parameters that receive large gradients. This can help the optimizer to converge faster and more efficiently, but can also lead to the learning rate becoming very small for some parameters and slowing down the convergence.\n",
    "* **Adam (Adaptive Moment Estimation)**: Adam is an optimization algorithm that combines the ideas of momentum and Adagrad. It uses an adaptive learning rate for each parameter, similar to Adagrad, but also takes into account the previous updates, similar to momentum. This allows Adam to converge faster and more efficiently than other optimizers, making it a popular choice for deep learning tasks.\n"
   ],
   "metadata": {
    "id": "65wi6yE8C_8K",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Putting it all together <a class=\"anchor\" id=\"example\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "Putting everything together, building a deep learning model with TensorFlow involves several steps. First, we need to prepare the data by loading it and preprocessing it as needed. Then we need to define the model architecture, including the input layer, hidden layers, and output layer, along with the appropriate activation functions for each layer. We also need to define the loss function and optimizer to be used for training.\n",
    "\n"
   ],
   "metadata": {
    "id": "fTP0TnacDoVw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_train.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jW_W_xzxEHJo",
    "outputId": "e5a30dc5-b3ae-4897-de2b-8d900e267bcd",
    "pycharm": {