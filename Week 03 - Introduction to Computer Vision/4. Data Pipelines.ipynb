
{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pipelines\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/TheAIDojo/AI_4_Climate_Bootcamp/blob/main/Week 03 - Introduction to Computer Vision/4. Data Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "TensorFlow data pipelines are a powerful tool that can simplify and optimize the process of data preparation, batching and loading for your machine learning models, particularly in the field of computer vision.\n",
        "\n",
        "In computer vision tasks, data preparation can be a time-consuming and memory-intensive process. The images used for training and validation are often very large in size and need to be preprocessed, resized and transformed into a format suitable for the model. The TensorFlow data pipeline helps automate these steps and makes it easier to use large datasets efficiently.\n",
        "\n",
        "TensorFlow data pipelines use the `tf.data` module, which provides a flexible and high-performance data pipeline that can handle a variety of data formats. With TensorFlow data pipelines, you can easily load and preprocess large datasets, and batch and shuffle the data for training your model. The pipeline also allows for parallel processing of the data, which can significantly speed up the training process.\n",
        "\n",
        "In this notebook, we will explore TensorFlow data pipelines in detail, and show you how to use them in your computer vision projects.\n",
        "\n",
        "Reference: \n",
        "- [TensorFlow Data Guide](https://www.tensorflow.org/guide/data) \n",
        "- [TensorFlow Image Loading and Preprocessing](https://www.tensorflow.org/tutorials/load_data/images)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents <a name=\"toc\"></a>\n",
        "* [Example on Dummy Data](#dummy)\n",
        "* [Data Pipe for Cats vs Dogs](#cats)\n",
        "  *  [Vanilla CNN Model](#vanilla)\n",
        "  *  [Transfer Learning ResNet50 Model](#transfer)\n",
        "* [Image Data Generatora](#generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "\n",
        "# The libraries below are built-into python and are used to work with local storage and directories\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example on Dummy Data <a name=\"dummy\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Let's start by creating a simple dataset of 10 numbers and then build a pipeline to process it. We will use the `tf.data.Dataset.from_tensor_slices()` method to create a dataset from a tensor. This method takes a tensor as input and returns a `tf.data.Dataset` object.\n",
        "\n",
        "This example is meant to illustrate the basic concepts of TensorFlow data pipelines. In practice, you will usually load your data from files or other sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1000, 5), (1000,))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create random dataset data using numpy\n",
        "x = np.random.uniform(low=0, high=1, size=(1000, 5))\n",
        "# create y using a linear formula with some noise\n",
        "y = (\n",
        "    -1 * x[:, 0]\n",
        "    + 1 * x[:, 1]\n",
        "    + -1 * x[:, 2]\n",
        "    + 1 * x[:, 3]\n",
        "    + -1 * x[:, 4]\n",
        "    + np.random.normal(loc=0, scale=0.1, size=1000)\n",
        ")\n",
        "\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create train and test sets\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    x, y, test_size=0.2, random_state=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M2 Pro\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-03 17:46:04.692598: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-02-03 17:46:04.692787: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# function to create tf.data.Dataset from numpy arrays\n",
        "def create_dataset(x, y):\n",
        "    # create tf.data.Dataset from numpy arrays\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    # shuffle the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=len(x))\n",
        "    # batch the dataset\n",
        "    dataset = dataset.batch(batch_size=8)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# create train and test datasets\n",
        "train_dataset = create_dataset(x_train, y_train)\n",
        "test_dataset = create_dataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 5) (8,)\n",
            "Inputs Batch: tf.Tensor(\n",
            "[[0.53157215 0.03265906 0.47559619 0.40774811 0.16121881]\n",
            " [0.31577315 0.0568814  0.76652732 0.70210745 0.33135222]\n",
            " [0.33424389 0.7709122  0.10659825 0.07513778 0.72818876]\n",
            " [0.29335342 0.52547031 0.69783279 0.90026411 0.79219119]\n",
            " [0.60754485 0.17052412 0.06505159 0.94888554 0.96563203]\n",
            " [0.58064523 0.93593725 0.67225069 0.48127795 0.80967504]\n",
            " [0.91985562 0.34634599 0.3469532  0.73750125 0.45221794]\n",
            " [0.18527685 0.54127011 0.64557218 0.04558516 0.94597089]], shape=(8, 5), dtype=float64)\n",
            "Outputs Batch: tf.Tensor(\n",
            "[-0.78881507 -0.67168966 -0.23385055 -0.4759486  -0.54681815 -0.71925862\n",
            " -0.49952118 -1.2057952 ], shape=(8,), dtype=float64)\n"
          ]
        }
      ],
      "source": [
        "# iterate over the dataset, notice that the dataset return batches of data and not single examples\n",
        "for x_batch, y_batch in train_dataset:\n",
        "    print(x_batch.shape, y_batch.shape)\n",
        "    print(\"Inputs Batch:\", x_batch)\n",
        "    print(\"Outputs Batch:\", y_batch)\n",
        "    break  # break the loop after the first iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create and train a super simple model\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(5,))])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
        ")\n",
        "model.fit(train_dataset, epochs=100, validation_data=test_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pipelines for Cats vs. Dogs Dataset <a name=\"cats\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Now let's look at a more realistic example. We will use the [Cats vs. Dogs dataset](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs). This dataset contains 2,000 images of cats and dogs, split into 1,000 training images and 1,000 validation images. The images are in JPEG format and have different sizes.\n",
        "\n",
        "Let's download the dataset and take a look at the images. We will notice that the images have different sizes and that the labels are stored in a separate file and folders. The downloaded dataset structure look something like this:\n",
        "\n",
        "```\n",
        "cats_vs_dogs_filtered/\n",
        "    train/\n",
        "        cats/\n",
        "            cat0001.jpg\n",
        "            cat0002.jpg\n",
        "            ...\n",
        "        dogs/\n",
        "            dog0001.jpg\n",
        "            dog0002.jpg\n",
        "            ...\n",
        "    validation/\n",
        "        cats/\n",
        "            cat2001.jpg\n",
        "            cat2002.jpg\n",
        "            ...\n",
        "        dogs/\n",
        "            dog2001.jpg\n",
        "            dog2002.jpg\n",
        "            ...\n",
        "```\n",
        "\n",
        "Notice how the folder structure splits the images into training and validation sets, and how the images are stored in separate folders for each class. We will use the `os` module to get the list of files and store them as image paths and labels in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download and unzip the dataset\n",
        "!wget https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
        "!unzip cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now read the dataset manually using os and put it into a dataframe\n",
        "# we will create a function to do this so we can reuse it for both the training and validation sets\n",
        "\n",
        "\n",
        "def read_dataset(path):  # e.g. path = cats_and_dogs_filtered/train\n",
        "    x = []  # images\n",
        "    y = []  # labels\n",
        "\n",
        "    # we will use os.listdir to read the contents of the folder\n",
        "    labels = os.listdir(path)  # e.g. labels = [\"cats\", \"dogs\"]\n",
        "\n",
        "    # iterate over the labels\n",
        "    for label in labels:\n",
        "        # we join the label to the path to get the path to the class folder which contains the images\n",
        "        label_dir = os.path.join(path, label)  # e.g. cats_and_dogs_filtered/train/cat\n",
        "\n",
        "        # we will use os.listdir again to read the contents of the class folders (i.e., the images)\n",
        "        images = os.listdir(label_dir)  # e.g. [\"cat.100.jpg\", \"cat.101.jpg\", ...]\n",
        "\n",
        "        # iterate over the images\n",
        "        for image in os.listdir(label_dir):\n",
        "            # we join the image name to the path to get the path to the image\n",
        "            image_path = os.path.join(\n",
        "                label_dir, image\n",
        "            )  # e.g. cats_and_dogs_filtered/train/cats/cat.100.jpg\n",
        "\n",
        "            # append the image path and the label to the lists\n",
        "            x.append(image_path)\n",
        "            y.append(label)\n",
        "\n",
        "    # return a dataframe with the image paths and labels\n",
        "    return pd.DataFrame({\"image\": x, \"label\": y})\n",
        "\n",
        "\n",
        "# read the training and validation sets\n",
        "train_df = read_dataset(\"cats_and_dogs_filtered/train\")\n",
        "val_df = read_dataset(\"cats_and_dogs_filtered/validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cats_and_dogs_filtered/train/dogs/dog.775.jpg</td>\n",
              "      <td>dogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",